[["index.html", "Project in Data Analytics for Decision Making Chapter 1 Introduction", " Project in Data Analytics for Decision Making Manon Verjus and Yooby Gigandet 2022-05-17 Chapter 1 Introduction Our work for the course “Project in Data Analytics for Decision Making” is to predict the credit risk linked to customers for our client, a bank. To do so, we used the CRISP-DM method (CRoss Industry Standard Process for Data Mining): Business understanding: Credit risk is defined as the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the lender. By performing a credit risk analysis, the lender determines the borrower’s ability to meet debt obligations in order to cushion itself from losses. It is therefore important to efficiently classify the risks of the credit applications. Data understanding and preparation: We work on the german dataset. Modelling: For this part, we used several models. Evaluation: Deployment: "],["data-understanding-and-preparation.html", "Chapter 2 Data understanding and preparation 2.1 Dataset 2.2 Exploratory Data Analysis", " Chapter 2 Data understanding and preparation 2.1 Dataset To create a model that will predict whether a client application represents a risk or not, we work on a dataset from our client containing data on 1000 past credit applications, described by the following variables: CHK_ACCT: The checking account status of the applicant in Deutsche Mark (DM). DURATION: The duration of the credit in months. HISTORY: The credit history of the applicant. NEW_CAR: Purpose of the credit. USED_CAR: Purpose of the credit. FURNITURE: Purpose of the credit. RADIO/TV: Purpose of the credit. EDUCATION: Purpose of the credit. RETRAINING: Purpose of the credit. AMOUNT: The credit amount. SAV_ACCT: The average balance in savings account in Deutsche Mark (DM). EMPLOYMENT: If the applicant is employed and since how long. INSTALL_RATE: The installment rate as percentage of disposable income. MALE_DIV: If the applicant is male and divorced. MALE_SINGLE: If the applicant is male and single. MALE_MAR_or_WID: If the applicant is male, married or widowed. CO_APPLICANT: If the applicant has a co-applicant. GUARANTOR: If the applicant has a guarantor. PRESENT_RESIDENT: If the applicant is a resident and since how many years. REAL_ESTATE: If the applicant owns real estate. PROP-UNKN-NONE: If the applicant owns no property (or unknown). AGE: Age of the applicant. OTHER_INSTALL: If the applicant has other installment plan credit. RENT: If the applicant rents. OWN_RES: If the applicant owns residence. NUM_CREDITS: Number of existing credits of the applicant at our client bank. JOB: The nature of the applicant’s job. NUM_DEPENDENT: Number of people for whom liable to provide maintenance. TELEPHONE: If the applicant has a phone in his or her name. FOREIGN: If the applicant is a foreign worker. RESPONSE: If the credit application is rated as “Good” or “Bad”. 2.2 Exploratory Data Analysis In this part, we thoroughly explore the dataset to get a better understanding. 2.2.1 Inacurracies By doing an exploratory data analysis, we find some inaccuracies and in agreement with our client, we change them as follow: One observation of the variable “AGE”: 75 instead of 125 years old. One observation of the variable “EDUCATION”: 1 instead of -1. One observation of the variable “GUARANTOR”: 1 instead of 2. Then, we separated the database in 2 parts: the first part consider only the good clients, and the second part consider only the bad clients. 2.2.2 Unbalanced observations One interesting point is that the dataset is heavily unbalanced: 700 credit applications rated as good versus 300 credit applications rated as bad. We will need to balance the dataset for our modelling. To have a better visualization of our variables. We did some boxplot which show the distribution of the data. This first boxplot A shows that bad clients mostly asked for longer duration credit than good clients. This could be a raison why they received a refusal. The second boxplot B shows that the age of clients doesn not seem to be a key criteria but the amount of the credit (boxplot C) seems to be more important as most of the good clients asked for a smaller amount. Then, we explored the distribution of the financial variables. The first graph A present the situation with past credits. The behavior for good and bad clients is almost the same which does not allow us to make a clear difference. On the graph B we can see that most of the clients have small savings of even no savings at all. Most of the clients have already 1 or 2 credits at this bank. Almost none of them ask for a third or fourth credit. On the histogram D we see that the amount of money on the account not so important. Indeed, most of the good client do not even have a checking account. The graphs below displays fournitures and products owned by the clients. Most of the time the credits are not very favorable to finance products. However, new cars are more appreciated by banks than used cars and radio/tv are more financed than furnitures. As we can see on the graph before, most of the time, if the client ask a credit to pay education, the bank will refuse, even if the client is good. It is slightly the same for retraining. Most of the good client have a job for more than a year and are skilled employees. #&gt; $`1` #&gt; #&gt; $`2` #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;list&quot; &quot;ggarrange&quot; The graph A presents the qualification of client depending of the duration stays in their home. This criteria does not seem to be really indicative even if the most good clients live in their place since more than 4 years. The installment rate does not seem to impact the qualification of the client neither as most of the good and bad clients are rated by 4 (see graph B). Owning a real estate is not the key to be perceived as a good client neither. As the graph C prove, a lot of good clients do not have any real estate. But at the opposite having no property is a bad situation in the case of asking for a credit. The graphs show that most of the good clients own their residence and not rent but a guarantor is definitely not a must. About the demographic data of an applicant, clients are generally not male and single and not foreign workers. The population of clients basically do not have co-applicants and they do not have other installment plan neither for majority of them. The name’s telephone does not seem to be important as a lot of clients have and have not the telephone on their name. However, most of the clients are not liable to provide maintenance. We can see below the mean of good clients and the mean of bad clients. It is important to keep in account that the sample of good clients is much higher than the number of bad clients. The matrix below present the correlations between variables. To read in a easiest way, we separated the variables in categories. As before, the categories were created based on their area. The following part of the project is focus on defining a typical profile of a good client. The goal is to define an average good client based on the average of all the variables defined. The lollipop plot below present the mean profile to be qualified as a good client. From these results, most of them ask for a new car, rent their flat but have real estate. The good client in average has a quite good amount of money and has a guarantor. These results are quite different than what we presented before. The following plot shows a selection of the most important variables of our dataframe. The two main disadvantages of these methods are: The increasing overfitting risk when the number of observations is insufficient. The significant computation time when the number of variables is large. The result below displays that we seem to have 6 relevant variables: checking the account, the duration of the credit asked, the history of previous credits, the savings, the amount of money owned by the applicant and the real estate owned. #&gt; Warning in train.default(x, y, weights = w, ...): You are trying to do regression and your outcome only has two #&gt; possible values Are you trying to do classification? If so, use a 2 level factor as your outcome column. #&gt; Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in #&gt; resampled performance measures. #&gt; rpart variable importance #&gt; #&gt; only 20 most important variables shown (out of 30) #&gt; #&gt; Overall #&gt; CHK_ACCT 0.1141 #&gt; DURATION 0.0803 #&gt; HISTORY 0.0767 #&gt; SAV_ACCT 0.0684 #&gt; AMOUNT 0.0516 #&gt; REAL_ESTATE 0.0343 #&gt; JOB 0.0000 #&gt; MALE_MAR_or_WID 0.0000 #&gt; PRESENT_RESIDENT 0.0000 #&gt; TELEPHONE 0.0000 #&gt; RETRAINING 0.0000 #&gt; USED_CAR 0.0000 #&gt; FURNITURE 0.0000 #&gt; AGE 0.0000 #&gt; OTHER_INSTALL 0.0000 #&gt; GUARANTOR 0.0000 #&gt; FOREIGN 0.0000 #&gt; RADIO_TV 0.0000 #&gt; OWN_RES 0.0000 #&gt; NEW_CAR 0.0000 "],["models.html", "Chapter 3 Models 3.1 Splitting strategies and balancing 3.2 K-Nearest Neighbors (K-NN) 3.3 Naive Bayes 3.4 Logistic Regression 3.5 Trees 3.6 Neural Networks 3.7 Trees", " Chapter 3 Models Now that our data set is cleaned, we move on to the creation of models to predict our outcome. 3.1 Splitting strategies and balancing In machine learning, a method to measure the accuracy of the models is to split the data into a training and a test set. The first subset is a portion of our data set that is fed into the machine learning model to discover and learn patterns. The other subset is to test our model. We split our data set as follows: Training set: 80% of the data Test set: the remaining 20% of the data # Splitting set.seed(346) # Creation of the index index.tr &lt;- createDataPartition(y = GermanCredit$RESPONSE, p= 0.8, list = FALSE) GermanCredit.tr &lt;- GermanCredit[index.tr,] # Training set GermanCredit.te &lt;- GermanCredit[-index.tr,] # Testing set As said in the exploratory data analysis, we notice that our data is heavily unbalanced. Outcome Frequence 0 236 1 564 Therefore, any model that favors the majority will reach an higher accuracy. However, in our case, we need to make sure to rightly classify any credit applications to avoid losses. To do so, we use a method called sub-sampling that balance the observations. # Balancing n_no &lt;- min(table(GermanCredit.tr$RESPONSE)) GermanCredit.tr.no &lt;- filter(GermanCredit.tr, as_factor(RESPONSE)==0) GermanCredit.tr.yes &lt;- filter(GermanCredit.tr, as_factor(RESPONSE)==1) ## sub-sample 236 instances from the &quot;Good&quot; index.no &lt;- sample(size=n_no, x=1:nrow(GermanCredit.tr.no), replace=FALSE) ## Bind all the &quot;Bad&quot; and the sub-sampled &quot;Good&quot; GermanCredit.tr.subs &lt;- data.frame(rbind(GermanCredit.tr.no, GermanCredit.tr.yes[index.no,])) Outcome Frequence 0 236 1 236 3.2 K-Nearest Neighbors (K-NN) A K-Nearest Neighbors tries to predict the correct class for the test data by calculating the distances between the test data and all the training points. Then select a number, K, of the closest point of the test set (thus the name K-nearest neighbors). trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) search_grid &lt;- expand.grid(k = seq(1, 85, by = 1)) set.seed(346) knn_cv &lt;- train(as_factor(RESPONSE)~., data = GermanCredit.tr.subs, method = &quot;knn&quot;, trControl = trctrl, metric = &quot;Accuracy&quot;, tuneGrid = search_grid) We need to select the K number of points closest that give the optimal accuracy 3.3 Naive Bayes Bayes classifiers use a probabilistic approach trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) search_grid &lt;- expand.grid( usekernel = c(TRUE, FALSE), laplace = 0:5, adjust = seq(0, 5, by = 1) ) set.seed(346) naive_bayes &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;naive_bayes&quot;, trControl=trctrl, tuneGrid = search_grid) 3.4 Logistic Regression A logistic regression is a regression adapted to binary classification. We use a cross-validation method to train our model and choose the Akaïke Information Criterion (AIC) to select the variables. trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) set.seed(346) glm_aic &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;glmStepAIC&quot;, family=&quot;binomial&quot;, trControl=trctrl, trace=0) The resulting confusion matrix of the logistic regression: 3.5 Trees The trees represent a hierarchical set of binary rules in a shape of a tree. #&gt; Obs #&gt; Pred 0 1 #&gt; 0 42 48 #&gt; 1 22 88 #&gt; 0 1 #&gt; 1 0.257 0.743 #&gt; 2 0.683 0.317 #&gt; 3 0.683 0.317 #&gt; 4 0.343 0.657 #&gt; 5 0.683 0.317 #&gt; 6 0.683 0.317 #&gt; 7 0.257 0.743 #&gt; 8 0.257 0.743 #&gt; 9 0.683 0.317 #&gt; 10 0.683 0.317 #&gt; 11 0.257 0.743 #&gt; 12 0.257 0.743 #&gt; 13 0.683 0.317 #&gt; 14 0.683 0.317 #&gt; 15 0.257 0.743 #&gt; 16 0.683 0.317 #&gt; 17 0.683 0.317 #&gt; 18 0.683 0.317 #&gt; 19 0.257 0.743 #&gt; 20 0.343 0.657 #&gt; 21 0.683 0.317 #&gt; 22 0.683 0.317 #&gt; 23 0.683 0.317 #&gt; 24 0.257 0.743 #&gt; 25 0.257 0.743 #&gt; 26 0.257 0.743 #&gt; 27 0.683 0.317 #&gt; 28 0.257 0.743 #&gt; 29 0.683 0.317 #&gt; 30 0.683 0.317 #&gt; 31 0.257 0.743 #&gt; 32 0.257 0.743 #&gt; 33 0.683 0.317 #&gt; 34 0.257 0.743 #&gt; 35 0.257 0.743 #&gt; 36 0.343 0.657 #&gt; 37 0.683 0.317 #&gt; 38 0.257 0.743 #&gt; 39 0.343 0.657 #&gt; 40 0.257 0.743 #&gt; 41 0.683 0.317 #&gt; 42 0.683 0.317 #&gt; 43 0.257 0.743 #&gt; 44 0.683 0.317 #&gt; 45 0.257 0.743 #&gt; 46 0.257 0.743 #&gt; 47 0.257 0.743 #&gt; 48 0.343 0.657 #&gt; 49 0.683 0.317 #&gt; 50 0.257 0.743 #&gt; 51 0.257 0.743 #&gt; 52 0.683 0.317 #&gt; 53 0.257 0.743 #&gt; 54 0.257 0.743 #&gt; 55 0.257 0.743 #&gt; 56 0.257 0.743 #&gt; 57 0.683 0.317 #&gt; 58 0.683 0.317 #&gt; 59 0.257 0.743 #&gt; 60 0.257 0.743 #&gt; 61 0.683 0.317 #&gt; 62 0.257 0.743 #&gt; 63 0.257 0.743 #&gt; 64 0.257 0.743 #&gt; 65 0.683 0.317 #&gt; 66 0.683 0.317 #&gt; 67 0.683 0.317 #&gt; 68 0.683 0.317 #&gt; 69 0.683 0.317 #&gt; 70 0.683 0.317 #&gt; 71 0.257 0.743 #&gt; 72 0.257 0.743 #&gt; 73 0.683 0.317 #&gt; 74 0.683 0.317 #&gt; 75 0.257 0.743 #&gt; 76 0.343 0.657 #&gt; 77 0.257 0.743 #&gt; 78 0.683 0.317 #&gt; 79 0.257 0.743 #&gt; 80 0.683 0.317 #&gt; 81 0.257 0.743 #&gt; 82 0.257 0.743 #&gt; 83 0.683 0.317 #&gt; 84 0.683 0.317 #&gt; 85 0.683 0.317 #&gt; 86 0.683 0.317 #&gt; 87 0.343 0.657 #&gt; 88 0.683 0.317 #&gt; 89 0.257 0.743 #&gt; 90 0.683 0.317 #&gt; 91 0.257 0.743 #&gt; 92 0.257 0.743 #&gt; 93 0.257 0.743 #&gt; 94 0.257 0.743 #&gt; 95 0.683 0.317 #&gt; 96 0.257 0.743 #&gt; 97 0.683 0.317 #&gt; 98 0.257 0.743 #&gt; 99 0.343 0.657 #&gt; 100 0.257 0.743 #&gt; 101 0.257 0.743 #&gt; 102 0.343 0.657 #&gt; 103 0.257 0.743 #&gt; 104 0.343 0.657 #&gt; 105 0.257 0.743 #&gt; 106 0.683 0.317 #&gt; 107 0.257 0.743 #&gt; 108 0.343 0.657 #&gt; 109 0.683 0.317 #&gt; 110 0.257 0.743 #&gt; 111 0.683 0.317 #&gt; 112 0.257 0.743 #&gt; 113 0.343 0.657 #&gt; 114 0.683 0.317 #&gt; 115 0.257 0.743 #&gt; 116 0.683 0.317 #&gt; 117 0.683 0.317 #&gt; 118 0.257 0.743 #&gt; 119 0.343 0.657 #&gt; 120 0.683 0.317 #&gt; 121 0.683 0.317 #&gt; 122 0.683 0.317 #&gt; 123 0.343 0.657 #&gt; 124 0.343 0.657 #&gt; 125 0.257 0.743 #&gt; 126 0.683 0.317 #&gt; 127 0.683 0.317 #&gt; 128 0.683 0.317 #&gt; 129 0.343 0.657 #&gt; 130 0.257 0.743 #&gt; 131 0.257 0.743 #&gt; 132 0.257 0.743 #&gt; 133 0.257 0.743 #&gt; 134 0.683 0.317 #&gt; 135 0.683 0.317 #&gt; 136 0.257 0.743 #&gt; 137 0.257 0.743 #&gt; 138 0.257 0.743 #&gt; 139 0.683 0.317 #&gt; 140 0.257 0.743 #&gt; 141 0.257 0.743 #&gt; 142 0.257 0.743 #&gt; 143 0.257 0.743 #&gt; 144 0.257 0.743 #&gt; 145 0.257 0.743 #&gt; 146 0.683 0.317 #&gt; 147 0.683 0.317 #&gt; 148 0.683 0.317 #&gt; 149 0.257 0.743 #&gt; 150 0.683 0.317 #&gt; 151 0.257 0.743 #&gt; 152 0.683 0.317 #&gt; 153 0.257 0.743 #&gt; 154 0.257 0.743 #&gt; 155 0.683 0.317 #&gt; 156 0.683 0.317 #&gt; 157 0.683 0.317 #&gt; 158 0.343 0.657 #&gt; 159 0.683 0.317 #&gt; 160 0.683 0.317 #&gt; 161 0.683 0.317 #&gt; 162 0.683 0.317 #&gt; 163 0.683 0.317 #&gt; 164 0.257 0.743 #&gt; 165 0.683 0.317 #&gt; 166 0.257 0.743 #&gt; 167 0.257 0.743 #&gt; 168 0.683 0.317 #&gt; 169 0.257 0.743 #&gt; 170 0.683 0.317 #&gt; 171 0.683 0.317 #&gt; 172 0.257 0.743 #&gt; 173 0.343 0.657 #&gt; 174 0.257 0.743 #&gt; 175 0.257 0.743 #&gt; 176 0.683 0.317 #&gt; 177 0.257 0.743 #&gt; 178 0.257 0.743 #&gt; 179 0.257 0.743 #&gt; 180 0.683 0.317 #&gt; 181 0.683 0.317 #&gt; 182 0.683 0.317 #&gt; 183 0.683 0.317 #&gt; 184 0.343 0.657 #&gt; 185 0.343 0.657 #&gt; 186 0.683 0.317 #&gt; 187 0.257 0.743 #&gt; 188 0.257 0.743 #&gt; 189 0.683 0.317 #&gt; 190 0.683 0.317 #&gt; 191 0.683 0.317 #&gt; 192 0.257 0.743 #&gt; 193 0.683 0.317 #&gt; 194 0.257 0.743 #&gt; 195 0.257 0.743 #&gt; 196 0.257 0.743 #&gt; 197 0.683 0.317 #&gt; 198 0.683 0.317 #&gt; 199 0.257 0.743 #&gt; 200 0.683 0.317 3.6 Neural Networks 3.7 Trees #&gt; Obs #&gt; Pred 0 1 #&gt; 0 42 48 #&gt; 1 22 88 #&gt; 0 1 #&gt; 1 0.257 0.743 #&gt; 2 0.683 0.317 #&gt; 3 0.683 0.317 #&gt; 4 0.343 0.657 #&gt; 5 0.683 0.317 #&gt; 6 0.683 0.317 #&gt; 7 0.257 0.743 #&gt; 8 0.257 0.743 #&gt; 9 0.683 0.317 #&gt; 10 0.683 0.317 #&gt; 11 0.257 0.743 #&gt; 12 0.257 0.743 #&gt; 13 0.683 0.317 #&gt; 14 0.683 0.317 #&gt; 15 0.257 0.743 #&gt; 16 0.683 0.317 #&gt; 17 0.683 0.317 #&gt; 18 0.683 0.317 #&gt; 19 0.257 0.743 #&gt; 20 0.343 0.657 #&gt; 21 0.683 0.317 #&gt; 22 0.683 0.317 #&gt; 23 0.683 0.317 #&gt; 24 0.257 0.743 #&gt; 25 0.257 0.743 #&gt; 26 0.257 0.743 #&gt; 27 0.683 0.317 #&gt; 28 0.257 0.743 #&gt; 29 0.683 0.317 #&gt; 30 0.683 0.317 #&gt; 31 0.257 0.743 #&gt; 32 0.257 0.743 #&gt; 33 0.683 0.317 #&gt; 34 0.257 0.743 #&gt; 35 0.257 0.743 #&gt; 36 0.343 0.657 #&gt; 37 0.683 0.317 #&gt; 38 0.257 0.743 #&gt; 39 0.343 0.657 #&gt; 40 0.257 0.743 #&gt; 41 0.683 0.317 #&gt; 42 0.683 0.317 #&gt; 43 0.257 0.743 #&gt; 44 0.683 0.317 #&gt; 45 0.257 0.743 #&gt; 46 0.257 0.743 #&gt; 47 0.257 0.743 #&gt; 48 0.343 0.657 #&gt; 49 0.683 0.317 #&gt; 50 0.257 0.743 #&gt; 51 0.257 0.743 #&gt; 52 0.683 0.317 #&gt; 53 0.257 0.743 #&gt; 54 0.257 0.743 #&gt; 55 0.257 0.743 #&gt; 56 0.257 0.743 #&gt; 57 0.683 0.317 #&gt; 58 0.683 0.317 #&gt; 59 0.257 0.743 #&gt; 60 0.257 0.743 #&gt; 61 0.683 0.317 #&gt; 62 0.257 0.743 #&gt; 63 0.257 0.743 #&gt; 64 0.257 0.743 #&gt; 65 0.683 0.317 #&gt; 66 0.683 0.317 #&gt; 67 0.683 0.317 #&gt; 68 0.683 0.317 #&gt; 69 0.683 0.317 #&gt; 70 0.683 0.317 #&gt; 71 0.257 0.743 #&gt; 72 0.257 0.743 #&gt; 73 0.683 0.317 #&gt; 74 0.683 0.317 #&gt; 75 0.257 0.743 #&gt; 76 0.343 0.657 #&gt; 77 0.257 0.743 #&gt; 78 0.683 0.317 #&gt; 79 0.257 0.743 #&gt; 80 0.683 0.317 #&gt; 81 0.257 0.743 #&gt; 82 0.257 0.743 #&gt; 83 0.683 0.317 #&gt; 84 0.683 0.317 #&gt; 85 0.683 0.317 #&gt; 86 0.683 0.317 #&gt; 87 0.343 0.657 #&gt; 88 0.683 0.317 #&gt; 89 0.257 0.743 #&gt; 90 0.683 0.317 #&gt; 91 0.257 0.743 #&gt; 92 0.257 0.743 #&gt; 93 0.257 0.743 #&gt; 94 0.257 0.743 #&gt; 95 0.683 0.317 #&gt; 96 0.257 0.743 #&gt; 97 0.683 0.317 #&gt; 98 0.257 0.743 #&gt; 99 0.343 0.657 #&gt; 100 0.257 0.743 #&gt; 101 0.257 0.743 #&gt; 102 0.343 0.657 #&gt; 103 0.257 0.743 #&gt; 104 0.343 0.657 #&gt; 105 0.257 0.743 #&gt; 106 0.683 0.317 #&gt; 107 0.257 0.743 #&gt; 108 0.343 0.657 #&gt; 109 0.683 0.317 #&gt; 110 0.257 0.743 #&gt; 111 0.683 0.317 #&gt; 112 0.257 0.743 #&gt; 113 0.343 0.657 #&gt; 114 0.683 0.317 #&gt; 115 0.257 0.743 #&gt; 116 0.683 0.317 #&gt; 117 0.683 0.317 #&gt; 118 0.257 0.743 #&gt; 119 0.343 0.657 #&gt; 120 0.683 0.317 #&gt; 121 0.683 0.317 #&gt; 122 0.683 0.317 #&gt; 123 0.343 0.657 #&gt; 124 0.343 0.657 #&gt; 125 0.257 0.743 #&gt; 126 0.683 0.317 #&gt; 127 0.683 0.317 #&gt; 128 0.683 0.317 #&gt; 129 0.343 0.657 #&gt; 130 0.257 0.743 #&gt; 131 0.257 0.743 #&gt; 132 0.257 0.743 #&gt; 133 0.257 0.743 #&gt; 134 0.683 0.317 #&gt; 135 0.683 0.317 #&gt; 136 0.257 0.743 #&gt; 137 0.257 0.743 #&gt; 138 0.257 0.743 #&gt; 139 0.683 0.317 #&gt; 140 0.257 0.743 #&gt; 141 0.257 0.743 #&gt; 142 0.257 0.743 #&gt; 143 0.257 0.743 #&gt; 144 0.257 0.743 #&gt; 145 0.257 0.743 #&gt; 146 0.683 0.317 #&gt; 147 0.683 0.317 #&gt; 148 0.683 0.317 #&gt; 149 0.257 0.743 #&gt; 150 0.683 0.317 #&gt; 151 0.257 0.743 #&gt; 152 0.683 0.317 #&gt; 153 0.257 0.743 #&gt; 154 0.257 0.743 #&gt; 155 0.683 0.317 #&gt; 156 0.683 0.317 #&gt; 157 0.683 0.317 #&gt; 158 0.343 0.657 #&gt; 159 0.683 0.317 #&gt; 160 0.683 0.317 #&gt; 161 0.683 0.317 #&gt; 162 0.683 0.317 #&gt; 163 0.683 0.317 #&gt; 164 0.257 0.743 #&gt; 165 0.683 0.317 #&gt; 166 0.257 0.743 #&gt; 167 0.257 0.743 #&gt; 168 0.683 0.317 #&gt; 169 0.257 0.743 #&gt; 170 0.683 0.317 #&gt; 171 0.683 0.317 #&gt; 172 0.257 0.743 #&gt; 173 0.343 0.657 #&gt; 174 0.257 0.743 #&gt; 175 0.257 0.743 #&gt; 176 0.683 0.317 #&gt; 177 0.257 0.743 #&gt; 178 0.257 0.743 #&gt; 179 0.257 0.743 #&gt; 180 0.683 0.317 #&gt; 181 0.683 0.317 #&gt; 182 0.683 0.317 #&gt; 183 0.683 0.317 #&gt; 184 0.343 0.657 #&gt; 185 0.343 0.657 #&gt; 186 0.683 0.317 #&gt; 187 0.257 0.743 #&gt; 188 0.257 0.743 #&gt; 189 0.683 0.317 #&gt; 190 0.683 0.317 #&gt; 191 0.683 0.317 #&gt; 192 0.257 0.743 #&gt; 193 0.683 0.317 #&gt; 194 0.257 0.743 #&gt; 195 0.257 0.743 #&gt; 196 0.257 0.743 #&gt; 197 0.683 0.317 #&gt; 198 0.683 0.317 #&gt; 199 0.257 0.743 #&gt; 200 0.683 0.317 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
