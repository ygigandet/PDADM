[["index.html", "Project in Data Analytics for Decision Making Chapter 1 Introduction", " Project in Data Analytics for Decision Making Manon Verjus and Yooby Gigandet 2022-06-06 Chapter 1 Introduction Our work for the course “Project in Data Analytics for Decision Making” is to predict the credit risk linked to customers for our client, a German bank. To do so, we used the CRISP-DM method (CRoss Industry Standard Process for Data Mining): Business understanding: Credit risk is defined as the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the lender. By performing a credit risk analysis, the lender determines the borrower’s ability to meet debt obligations in order to cushion itself from losses. It is therefore important to efficiently classify the risks of the credit applications. Data understanding and preparation: We study the german data set. This part focus on visualization of the data set. We observed the variables, the distribution of the data through plots and tables. The goal of this part is to well understand the data set we work with to be able to find relevant model later. Modelling: After observing our data set, we need to find which model is the most relevant for the analysis. We fitted several models to assess which one(s) is(are) the best. Evaluation: By testing several model, we can sum the output and decide the most relevant model to use. The evaluation part focuses on the application of the chosen model(s). Deployment: To finish this study, we will assess some proposals and hypothesis on the customer profiles to avoid or to favor to minimize the risk in the bank. "],["data-understanding-and-preparation.html", "Chapter 2 Data understanding and preparation 2.1 Dataset 2.2 Exploratory Data Analysis", " Chapter 2 Data understanding and preparation 2.1 Dataset To create a model that will predict whether a client application represents a risk or not, we worked on a data set from our client containing data on 1000 past credit applications, described by the following variables: CHK_ACCT: The checking account status of the applicant in Deutsche Mark (DM). DURATION: The duration of the credit in months. HISTORY: The credit history of the applicant. NEW_CAR: Purpose of the credit. USED_CAR: Purpose of the credit. FURNITURE: Purpose of the credit. RADIO/TV: Purpose of the credit. EDUCATION: Purpose of the credit. RETRAINING: Purpose of the credit. AMOUNT: The credit amount. SAV_ACCT: The average balance in savings account in Deutsche Mark (DM). EMPLOYMENT: If the applicant is employed and since how long. INSTALL_RATE: The installment rate as percentage of disposable income. MALE_DIV: If the applicant is male and divorced. MALE_SINGLE: If the applicant is male and single. MALE_MAR_or_WID: If the applicant is male, married or widowed. CO_APPLICANT: If the applicant has a co-applicant. GUARANTOR: If the applicant has a guarantor. PRESENT_RESIDENT: If the applicant is a resident and since how many years. REAL_ESTATE: If the applicant owns real estate. PROP-UNKN-NONE: If the applicant owns no property (or unknown). AGE: Age of the applicant. OTHER_INSTALL: If the applicant has other installment plan credit. RENT: If the applicant rents. OWN_RES: If the applicant owns residence. NUM_CREDITS: Number of existing credits of the applicant at our client bank. JOB: The nature of the applicant’s job. NUM_DEPENDENT: Number of people for whom liable to provide maintenance. TELEPHONE: If the applicant has a phone in his or her name. FOREIGN: If the applicant is a foreign worker. RESPONSE: If the credit application is rated as “Good” or “Bad”. 2.2 Exploratory Data Analysis In this part, we thoroughly explored the data set to get a better understanding. The goal of the exploratory data analysis was to observe and interpret our data thanks to visualization methods, like plots. 2.2.1 Inacurracies By doing an exploratory data analysis, we found some inaccuracies and in agreement with our client, we changed them as follow: One observation of the variable “AGE”: 75 instead of 125 years old. One observation of the variable “EDUCATION”: 1 instead of -1. One observation of the variable “GUARANTOR”: 1 instead of 2. 2.2.2 Unbalanced observations To start, we noticed an important point to consider: the data set is heavily unbalanced. As you can see on the following plot, it contains 700 credit applications rated as good versus 300 credit applications rated as bad. We will need to balance the data set for our modelling. Taking the last point into account, we also decided to split the database in 2 parts: the first part considered only the good clients, and the second part considered only the bad clients. This will help us to study the particularities of each profile. 2.2.3 Visualization We chose to show some specifics variables as they are the ones we will be using in the modelling part. To have a better visualization of our variables. We used some box plots or histograms showing the distribution of the data. This first box plot A shows that bad clients mostly asked for longer duration credit than good clients. This could be a reason why they received a refusal. The second box plot B shows that most of the good clients asked for a smaller amount. Then, we explored the distribution of the financial variables: The first graph A represents the situation with past credits. The behavior for good and bad clients is almost the same which does not allow us to make a clear difference. On the graph B we can see that most of the clients have small savings or even no savings at all. Most of the clients have already 1 or 2 credits at this bank. Almost none of them ask for a third or fourth credit. On the histogram D we see that the amount of money on the account not so important. Indeed, most of the good client do not even have a checking account. 2.2.4 Profile of a good client The following part of the project is focus on defining a typical profile of a good client. The goal is to define an average good client based on the average of all the variables defined. First, we cleaned the data set created at the beginning which contain only good clients to have the average of each variable. Then, we created a plot with these average data. The lollipop plot below represents the mean profile to be qualified as a good client. From these results, most of them asked for a new car, rented their flat but had real estate. The good client in average had a quite good amount of money and had a guarantor. These results are quite different than what we presented before. s 2.2.5 Profile of a bad client Then, we used exactly the same approach to build the typical profile of a bad client and identify the average variables that qualify a bad client in this bank. As we can see on the orange lollipop plot, the result is different. Most of the bad clients rented their flat. This criteria sounds really important to qualify a client. Almost all of them do not have any other credit in the bank and they have other installment plan credits. "],["modelling.html", "Chapter 3 Modelling 3.1 Splitting strategies and balancing 3.2 K-Nearest Neighbors (K-NN) 3.3 Naive Bayes 3.4 Logistic Regression 3.5 Linear Discriminant Analysis 3.6 Trees 3.7 Neural Network 3.8 Random Forest 3.9 Summary 3.10 Variable Importance", " Chapter 3 Modelling After cleaning and visualizing the data set, we move on to the creation of models. 3.1 Splitting strategies and balancing In machine learning, a method to measure the accuracy of the models is to split the data into a training and a test set. The first subset is a portion of our data set that is fed into the machine learning model to discover and learn patterns. The other subset is to test our model. We split our data set as follows: Training set: 80% of the data Test set: the remaining 20% of the data # Splitting set.seed(346) # Creation of the index index.tr &lt;- createDataPartition(y = GermanCredit$RESPONSE, p= 0.8, list = FALSE) GermanCredit.tr &lt;- GermanCredit[index.tr,] # Training set GermanCredit.te &lt;- GermanCredit[-index.tr,] # Testing set As said in the exploratory data analysis, we notice that our data is heavily unbalanced. Outcome Frequence 0 236 1 564 Therefore, any model that favors the majority will reach an higher accuracy. However, in our case, we need to make sure to rightly classify any credit applications to avoid losses. To do so, we use a method called sub-sampling that balance the observations. It will allow us to have two equivalent samples. # Balancing n_no &lt;- min(table(GermanCredit.tr$RESPONSE)) GermanCredit.tr.no &lt;- filter(GermanCredit.tr, as_factor(RESPONSE)==0) GermanCredit.tr.yes &lt;- filter(GermanCredit.tr, as_factor(RESPONSE)==1) ## sub-sample 236 instances from the &quot;Good&quot; index.no &lt;- sample(size=n_no, x=1:nrow(GermanCredit.tr.no), replace=FALSE) ## Bind all the &quot;Bad&quot; and the sub-sampled &quot;Good&quot; GermanCredit.tr.subs &lt;- data.frame(rbind(GermanCredit.tr.no, GermanCredit.tr.yes[index.no,])) By doing so, we get the following sub sampled training set: Outcome Frequence 0 236 1 236 3.2 K-Nearest Neighbors (K-NN) A K-Nearest Neighbors tries to predict the correct class for the test data by calculating the distances between the test data and all the training points. Then to predict, it selects a number K, of the closest point of the test set (thus the name K-nearest neighbors). # K-Nearest Neighbors (K-NN) trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) # Cross-validation search_grid &lt;- expand.grid(k = seq(1, 85, by = 1)) set.seed(346) knn_cv &lt;- train(as_factor(RESPONSE)~., data = GermanCredit.tr.subs, method = &quot;knn&quot;, trControl = trctrl, metric = &quot;Accuracy&quot;, tuneGrid = search_grid) We need to select the K number of points closest that give the optimal accuracy. In this case, it is 77 We did several confusion matrix to measure the performance. The first one was with the K-NNk method. We found an accuracy of 0.615 which corresponds to well predicted clients (good and bad). The best accuracy value is 1 so 0.615 is considered as an average value. 3.3 Naive Bayes Then, we used a probabilistic approach with Bayes classifiers. With this method, the confusion matrix provides a better accuracy of 0.72. # Naive Bayes trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) search_grid &lt;- expand.grid( usekernel = c(TRUE, FALSE), laplace = 0:5, adjust = seq(0, 5, by = 1) ) set.seed(346) naive_bayes &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;naive_bayes&quot;, trControl=trctrl, tuneGrid = search_grid) 3.4 Logistic Regression A logistic regression is a regression adapted to binary classification. We use a cross-validation method to train our model and choose the Akaïke Information Criterion (AIC) to select the variables. The AIC is used to select the model based on the number of parameters. We choose the model with the smallest AIC. # Logistic Regression trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) set.seed(346) glm_aic &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;glmStepAIC&quot;, family=&quot;binomial&quot;, trControl=trctrl, trace=0) The following plot corresponds to the resulting confusion matrix of the logistic regression: It provides an accuracy of 0.73. 3.5 Linear Discriminant Analysis # Linear Discriminant Analysis trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) set.seed(346) lda.model &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;lda&quot;, trControl = trctrl) 3.6 Trees The trees represent a hierarchical set of binary rules in a shape of a tree. # Trees trctrl &lt;- trainControl(method = &quot;cv&quot;, number=10) search_grid &lt;- expand.grid(cp = seq(from = 0.1, to = 0, by = -0.01)) set.seed(346) tree_model &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;rpart&quot;, trControl=trctrl, tuneGrid = search_grid) 3.7 Neural Network Neural Network is a method based on combining several predictions of small nodes. # Neural Network (NN) trctrl &lt;- trainControl(method = &quot;cv&quot;, number=5) search_grid &lt;- expand.grid(size = 1:10, decay = seq(0, 0.5, 0.1)) set.seed(346) neural_network &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;nnet&quot;, trControl=trctrl, tuneGrid = search_grid) The confusion matrix with neural network presents a good accuracy of 0.705. 3.8 Random Forest # Random Forest trctrl &lt;- trainControl(method = &quot;cv&quot;, number=5) search_grid &lt;- expand.grid(.mtry = c(1:15)) set.seed(346) rf &lt;- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = &quot;rf&quot;, trControl = trctrl, tuneGrid = search_grid ) Again, this models shares an accuracy of 0.73. 3.9 Summary To evaluate our models, there are many metrics that can be used. We decided to choose first the accuracy and then have a look at the sensitivity and the specificity. In other words, we want to have a model with a good accuracy and the best sensitivity and specificity. Accuracy Sensitivity Specificity Tree 0.74 0.66 0.65 Logistic Regression 0.73 0.72 0.74 Random Forest 0.73 0.69 0.75 Linear Discriminant Analysis 0.72 0.75 0.71 Naive Bayes 0.72 0.81 0.68 Neural Network 0.70 0.75 0.68 K-nearest neighbors 0.62 0.47 0.68 By looking at this summary table of the models, we can see that the Tree, the Logistic Regression and the Random Forest are our top models. However, the tree has a lowest score in terms of specificity and sensitivity than the two others and by far. For this reason, we decided to choose the following models: Logistic Regression Random Forest 3.10 Variable Importance The following plot shows a selection of the most important variables of our data frame. The two main disadvantages of these methods are: The increasing over fitting risk when the number of observations is insufficient. The significant computation time when the number of variables is large. The result below displays that we seem to have 6 relevant variables: checking account, the duration of the credit asked, the history of previous credits, the savings, the amount of money owned by the applicant and the real estate owned. After this first global analysis, we wanted to evaluate if there is room for improvement for the models selected in Machine Learning, in other words, if we can remove some variables to make the analysis easier without loosing the performance. 3.10.1 Logistic Regression Accuracy Sensitivity Specificity Original Logistic Regression 0.73 0.72 0.74 Refit Logistic Regression 0.66 0.61 0.68 As we can see in the table above, we lose performance in terms of the metrics. This means that the variable importance is not that important for the logistic regression. A good point is that we need to remind ourselves that the logistic regression already does a variable importance with the AIC criterion. 3.10.2 Random Forest Accuracy Sensitivity Specificity Original Random Forest 0.73 0.69 0.75 Refit Random Forest 0.68 0.70 0.67 Again, we lose a bit of performance in terms of the metrics. However, the model is simpler and might be easier to understand. "],["evaluation-and-deployment.html", "Chapter 4 Evaluation and deployment 4.1 Evaluation 4.2 Deployment", " Chapter 4 Evaluation and deployment 4.1 Evaluation In this part, we wanted to evaluate the work done so far. The idea was to provide a good model that can predict whether a client represents a risk or not for the bank based on the data provided. As seen in the modelization part, we found two good models with a good accuracy, or what we estimated as a good accuracy: The logistic regression: an accuracy of 73% (Specificity: 74%, sensitivity: 72%) The random forest: an accuracy of 73% (Specificty: 75%, sensitivity: 69%) Another part of our analysis was to provide simpler models that would only take into account some of the varaibles found in the data set (the most important ones). With these new models, they are indeed simpler but they lose a bit in term of performance: The logistic regression: an accuracy of 66% The random forest: an accuracy of 68% As we can see from the numbers above, our models are not perfect, which is also a good sign as a perfect model would be overfitting. In other words, would work perfectly with the data provided but would be poorly performing when facing new data. 4.2 Deployment If our client is interested to implement our solution, we might inform him to use it as an helping tool, giving a first input to the analysis of the bankers but absolutely not being an automatic process. Like the question asked above, the bankers should estimate which models would be better (the most performing one or the most easiest one). This is a huge difference as the first models might be more accurate but would required more data from each client to be able to predict correctly. On the other hand, the second model would give less accuracy but would need less data to give a result. It is hence at the hands of our client to choose which solution he prefers and assess it as an helping tool. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
