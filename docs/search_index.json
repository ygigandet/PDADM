[["index.html", "Project in Data Analytics for Decision Making Chapter 1 Introduction", " Project in Data Analytics for Decision Making Manon Verjus and Yooby Gigandet 2022-05-14 Chapter 1 Introduction Our work for the course “Project in Data Analytics for Decision Making” is to predict the credit risk associated to customers for our client, a bank. To do so, we used the CRISP-DM method (CRoss Industry Standard Process for Data Mining): Business understanding: Credit risk is defined as the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the lender. By performing a credit risk analysis, the lender determines the borrower’s ability to meet debt obligations in order to cushion itself from losses. It is therefore important to efficiently classify the risks of the credit applications. Data understanding and preparation: We work on the german dataset. Modelling: For this part, we used several models. Evaluation: Deployment: "],["data-understanding-and-preparation.html", "Chapter 2 Data understanding and preparation 2.1 Dataset 2.2 Exploratory Data Analysis", " Chapter 2 Data understanding and preparation 2.1 Dataset To create a model that will predict whether a client application represents a risk or not, we work on a dataset from our client containing data on 1000 past credit applications, described by the following variables: CHK_ACCT: The checking account status of the applicant in Deutsche Mark (DM). DURATION: The duration of the credit in months. HISTORY: The credit history of the applicant. NEW_CAR: Purpose of the credit. USED_CAR: Purpose of the credit. FURNITURE: Purpose of the credit. RADIO/TV: Purpose of the credit. EDUCATION: Purpose of the credit. RETRAINING: Purpose of the credit. AMOUNT: The credit amount. SAV_ACCT: The average balance in savings account in Deutsche Mark (DM). EMPLOYMENT: If the applicant is employed and since how long. INSTALL_RATE: The installment rate as percentage of disposable income. MALE_DIV: If the applicant is male and divorced. MALE_SINGLE: If the applicant is male and single. MALE_MAR_or_WID: If the applicant is male, married or widowed. CO_APPLICANT: If the applicant has a co-applicant. GUARANTOR: If the applicant has a guarantor. PRESENT_RESIDENT: If the applicant is a resident and since how many years. REAL_ESTATE: If the applicant owns real estate. PROP-UNKN-NONE: If the applicant owns no property (or unknown). AGE: Age of the applicant. OTHER_INSTALL: If the applicant has other installment plan credit. RENT: If the applicant rents. OWN_RES: If the applicant owns residence. NUM_CREDITS: Number of existing credits of the applicant at our client bank. JOB: The nature of the applicant’s job. NUM_DEPENDENT: Number of people for whom liable to provide maintenance. TELEPHONE: If the applicant has a phone in his or her name. FOREIGN: If the applicant is a foreign worker. RESPONSE: If the credit application is rated as “Good” or “Bad”. 2.2 Exploratory Data Analysis In this part, we thoroughly explore the dataset to get a better understanding. 2.2.1 Inacurracies By doing an exploratory data analysis, we find some inaccuracies and in agreement with our client, we change them as follow: One observation of the variable “AGE”: 75 instead of 125 years old. One observation of the variable “EDUCATION”: 1 instead of -1. One observation of the variable “GUARANTOR”: 1 instead of 2. 2.2.2 Unbalanced observations One interesting point is that the dataset is heavily unbalanced: 700 credit applications rated as good versus 300 credit applications rated as bad. We will need to balance the dataset for our modelling. #&gt; $`1` #&gt; #&gt; $`2` #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;list&quot; &quot;ggarrange&quot; The following plot shows a selection of the most important variables of our dataframe. The two main disadvantages of these methods are: The increasing overfitting risk when the number of observations is insufficient. The significant computation time when the number of variables is large. #&gt; Warning in train.default(x, y, weights = w, ...): You are trying to #&gt; do regression and your outcome only has two possible values Are you #&gt; trying to do classification? If so, use a 2 level factor as your #&gt; outcome column. #&gt; Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info #&gt; = trainInfo, : There were missing values in resampled performance #&gt; measures. #&gt; rpart variable importance #&gt; #&gt; only 20 most important variables shown (out of 30) #&gt; #&gt; Overall #&gt; CHK_ACCT 0.1141 #&gt; DURATION 0.0803 #&gt; HISTORY 0.0767 #&gt; SAV_ACCT 0.0684 #&gt; AMOUNT 0.0516 #&gt; REAL_ESTATE 0.0343 #&gt; JOB 0.0000 #&gt; MALE_MAR_or_WID 0.0000 #&gt; PRESENT_RESIDENT 0.0000 #&gt; TELEPHONE 0.0000 #&gt; RETRAINING 0.0000 #&gt; USED_CAR 0.0000 #&gt; FURNITURE 0.0000 #&gt; AGE 0.0000 #&gt; OTHER_INSTALL 0.0000 #&gt; GUARANTOR 0.0000 #&gt; FOREIGN 0.0000 #&gt; RADIO_TV 0.0000 #&gt; OWN_RES 0.0000 #&gt; NEW_CAR 0.0000 "],["models.html", "Chapter 3 Models 3.1 Splitting strategies 3.2 Balance the dataset 3.3 Logistic Regression 3.4 Neural Networks 3.5 Trees", " Chapter 3 Models After the exploration of our data, we start with the modelling part. 3.1 Splitting strategies In order to test the performance of our models, we use a splitting strategy of the dataset: 80% is used as a training set The remaining 20% is used as a test set. 3.2 Balance the dataset As said in the exploratory data analysis, we notice that our data is heavily unbalanced. We need to balance it before fitting our model. 3.3 Logistic Regression We fit a logistic regression to test if it could predict the output we desired. For our first model we’ll select all the variables given in the German Credit data set. Our initial model is therefore: \\[ z_i = a_o + a_1{CHKACCT}_{31i} + ... + a_{31}Foreign_{31i} \\] Selection of the model Final model: \\[ z_i = a_o + a_1{CHKACCT}_{1i} + a_2{DURATION}_{2i} + a_3{HISTORY}_{3i} + a_4{NEWCAR}_{4i} + a_4{USEDCAR}_{4i} + a_5{AMOUNT}_{5i} + a_6{SAVACCT}_{6i} + a_7{EMPLOYMENT}_{7i} + a_8{INSTALLRATE}_{8i} + a_9{MALSINGLE}_{9i} + a_10{CO-APPLICANT}_{10i} + a_11{GUARANTOR}_{11i} + a_12{REALESTATE}_{12i} + a_13{OTHERINSTALL}_{13i} + a_14{OWNRES}_{14i} + a_15{NUMCREDITS}_{15i} + a_16{NUMDEPENDENTS}_{16i} + a_17{FOREIGN}_{17i} \\] Predictions: Not a good model. Do not forget that the prior of this data set is not balanced at all. We should take it into account. As depicted below, if our model was good, we should have boxplot that are far away from the 0.5, which is not the case, especially for the bad credits. 3.3.1 Cross-validation 3.4 Neural Networks #&gt; # weights: 65 #&gt; initial value 328.042130 #&gt; final value 327.165469 #&gt; converged #&gt; a 30-2-1 network with 65 weights #&gt; options were - entropy fitting #&gt; b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 i5-&gt;h1 i6-&gt;h1 i7-&gt;h1 #&gt; -0.21 -0.20 0.09 0.08 -0.13 0.13 0.05 -0.53 #&gt; i8-&gt;h1 i9-&gt;h1 i10-&gt;h1 i11-&gt;h1 i12-&gt;h1 i13-&gt;h1 i14-&gt;h1 i15-&gt;h1 #&gt; -0.67 0.64 -0.43 -0.59 -0.68 -0.20 -0.25 -0.61 #&gt; i16-&gt;h1 i17-&gt;h1 i18-&gt;h1 i19-&gt;h1 i20-&gt;h1 i21-&gt;h1 i22-&gt;h1 i23-&gt;h1 #&gt; 0.12 -0.03 0.09 -0.02 -0.27 -0.35 -0.01 -0.36 #&gt; i24-&gt;h1 i25-&gt;h1 i26-&gt;h1 i27-&gt;h1 i28-&gt;h1 i29-&gt;h1 i30-&gt;h1 #&gt; -0.18 -0.60 -0.60 -0.48 0.22 0.61 -0.51 #&gt; b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 i5-&gt;h2 i6-&gt;h2 i7-&gt;h2 #&gt; 0.16 0.64 0.70 0.11 -0.09 -0.28 -0.58 -0.17 #&gt; i8-&gt;h2 i9-&gt;h2 i10-&gt;h2 i11-&gt;h2 i12-&gt;h2 i13-&gt;h2 i14-&gt;h2 i15-&gt;h2 #&gt; 0.48 -0.70 -0.35 -0.20 -0.19 -0.36 0.39 0.10 #&gt; i16-&gt;h2 i17-&gt;h2 i18-&gt;h2 i19-&gt;h2 i20-&gt;h2 i21-&gt;h2 i22-&gt;h2 i23-&gt;h2 #&gt; -0.15 -0.50 0.33 0.01 0.55 -0.41 0.43 0.54 #&gt; i24-&gt;h2 i25-&gt;h2 i26-&gt;h2 i27-&gt;h2 i28-&gt;h2 i29-&gt;h2 i30-&gt;h2 #&gt; 0.11 0.59 0.11 0.07 -0.05 0.62 0.36 #&gt; b-&gt;o h1-&gt;o h2-&gt;o #&gt; 0.00 0.55 -0.29 #&gt; [1] 0.5 3.5 Trees #&gt; Obs #&gt; Pred 0 1 #&gt; 0 42 48 #&gt; 1 22 88 #&gt; 0 1 #&gt; 1 0.257 0.743 #&gt; 2 0.683 0.317 #&gt; 3 0.683 0.317 #&gt; 4 0.343 0.657 #&gt; 5 0.683 0.317 #&gt; 6 0.683 0.317 #&gt; 7 0.257 0.743 #&gt; 8 0.257 0.743 #&gt; 9 0.683 0.317 #&gt; 10 0.683 0.317 #&gt; 11 0.257 0.743 #&gt; 12 0.257 0.743 #&gt; 13 0.683 0.317 #&gt; 14 0.683 0.317 #&gt; 15 0.257 0.743 #&gt; 16 0.683 0.317 #&gt; 17 0.683 0.317 #&gt; 18 0.683 0.317 #&gt; 19 0.257 0.743 #&gt; 20 0.343 0.657 #&gt; 21 0.683 0.317 #&gt; 22 0.683 0.317 #&gt; 23 0.683 0.317 #&gt; 24 0.257 0.743 #&gt; 25 0.257 0.743 #&gt; 26 0.257 0.743 #&gt; 27 0.683 0.317 #&gt; 28 0.257 0.743 #&gt; 29 0.683 0.317 #&gt; 30 0.683 0.317 #&gt; 31 0.257 0.743 #&gt; 32 0.257 0.743 #&gt; 33 0.683 0.317 #&gt; 34 0.257 0.743 #&gt; 35 0.257 0.743 #&gt; 36 0.343 0.657 #&gt; 37 0.683 0.317 #&gt; 38 0.257 0.743 #&gt; 39 0.343 0.657 #&gt; 40 0.257 0.743 #&gt; 41 0.683 0.317 #&gt; 42 0.683 0.317 #&gt; 43 0.257 0.743 #&gt; 44 0.683 0.317 #&gt; 45 0.257 0.743 #&gt; 46 0.257 0.743 #&gt; 47 0.257 0.743 #&gt; 48 0.343 0.657 #&gt; 49 0.683 0.317 #&gt; 50 0.257 0.743 #&gt; 51 0.257 0.743 #&gt; 52 0.683 0.317 #&gt; 53 0.257 0.743 #&gt; 54 0.257 0.743 #&gt; 55 0.257 0.743 #&gt; 56 0.257 0.743 #&gt; 57 0.683 0.317 #&gt; 58 0.683 0.317 #&gt; 59 0.257 0.743 #&gt; 60 0.257 0.743 #&gt; 61 0.683 0.317 #&gt; 62 0.257 0.743 #&gt; 63 0.257 0.743 #&gt; 64 0.257 0.743 #&gt; 65 0.683 0.317 #&gt; 66 0.683 0.317 #&gt; 67 0.683 0.317 #&gt; 68 0.683 0.317 #&gt; 69 0.683 0.317 #&gt; 70 0.683 0.317 #&gt; 71 0.257 0.743 #&gt; 72 0.257 0.743 #&gt; 73 0.683 0.317 #&gt; 74 0.683 0.317 #&gt; 75 0.257 0.743 #&gt; 76 0.343 0.657 #&gt; 77 0.257 0.743 #&gt; 78 0.683 0.317 #&gt; 79 0.257 0.743 #&gt; 80 0.683 0.317 #&gt; 81 0.257 0.743 #&gt; 82 0.257 0.743 #&gt; 83 0.683 0.317 #&gt; 84 0.683 0.317 #&gt; 85 0.683 0.317 #&gt; 86 0.683 0.317 #&gt; 87 0.343 0.657 #&gt; 88 0.683 0.317 #&gt; 89 0.257 0.743 #&gt; 90 0.683 0.317 #&gt; 91 0.257 0.743 #&gt; 92 0.257 0.743 #&gt; 93 0.257 0.743 #&gt; 94 0.257 0.743 #&gt; 95 0.683 0.317 #&gt; 96 0.257 0.743 #&gt; 97 0.683 0.317 #&gt; 98 0.257 0.743 #&gt; 99 0.343 0.657 #&gt; 100 0.257 0.743 #&gt; 101 0.257 0.743 #&gt; 102 0.343 0.657 #&gt; 103 0.257 0.743 #&gt; 104 0.343 0.657 #&gt; 105 0.257 0.743 #&gt; 106 0.683 0.317 #&gt; 107 0.257 0.743 #&gt; 108 0.343 0.657 #&gt; 109 0.683 0.317 #&gt; 110 0.257 0.743 #&gt; 111 0.683 0.317 #&gt; 112 0.257 0.743 #&gt; 113 0.343 0.657 #&gt; 114 0.683 0.317 #&gt; 115 0.257 0.743 #&gt; 116 0.683 0.317 #&gt; 117 0.683 0.317 #&gt; 118 0.257 0.743 #&gt; 119 0.343 0.657 #&gt; 120 0.683 0.317 #&gt; 121 0.683 0.317 #&gt; 122 0.683 0.317 #&gt; 123 0.343 0.657 #&gt; 124 0.343 0.657 #&gt; 125 0.257 0.743 #&gt; 126 0.683 0.317 #&gt; 127 0.683 0.317 #&gt; 128 0.683 0.317 #&gt; 129 0.343 0.657 #&gt; 130 0.257 0.743 #&gt; 131 0.257 0.743 #&gt; 132 0.257 0.743 #&gt; 133 0.257 0.743 #&gt; 134 0.683 0.317 #&gt; 135 0.683 0.317 #&gt; 136 0.257 0.743 #&gt; 137 0.257 0.743 #&gt; 138 0.257 0.743 #&gt; 139 0.683 0.317 #&gt; 140 0.257 0.743 #&gt; 141 0.257 0.743 #&gt; 142 0.257 0.743 #&gt; 143 0.257 0.743 #&gt; 144 0.257 0.743 #&gt; 145 0.257 0.743 #&gt; 146 0.683 0.317 #&gt; 147 0.683 0.317 #&gt; 148 0.683 0.317 #&gt; 149 0.257 0.743 #&gt; 150 0.683 0.317 #&gt; 151 0.257 0.743 #&gt; 152 0.683 0.317 #&gt; 153 0.257 0.743 #&gt; 154 0.257 0.743 #&gt; 155 0.683 0.317 #&gt; 156 0.683 0.317 #&gt; 157 0.683 0.317 #&gt; 158 0.343 0.657 #&gt; 159 0.683 0.317 #&gt; 160 0.683 0.317 #&gt; 161 0.683 0.317 #&gt; 162 0.683 0.317 #&gt; 163 0.683 0.317 #&gt; 164 0.257 0.743 #&gt; 165 0.683 0.317 #&gt; 166 0.257 0.743 #&gt; 167 0.257 0.743 #&gt; 168 0.683 0.317 #&gt; 169 0.257 0.743 #&gt; 170 0.683 0.317 #&gt; 171 0.683 0.317 #&gt; 172 0.257 0.743 #&gt; 173 0.343 0.657 #&gt; 174 0.257 0.743 #&gt; 175 0.257 0.743 #&gt; 176 0.683 0.317 #&gt; 177 0.257 0.743 #&gt; 178 0.257 0.743 #&gt; 179 0.257 0.743 #&gt; 180 0.683 0.317 #&gt; 181 0.683 0.317 #&gt; 182 0.683 0.317 #&gt; 183 0.683 0.317 #&gt; 184 0.343 0.657 #&gt; 185 0.343 0.657 #&gt; 186 0.683 0.317 #&gt; 187 0.257 0.743 #&gt; 188 0.257 0.743 #&gt; 189 0.683 0.317 #&gt; 190 0.683 0.317 #&gt; 191 0.683 0.317 #&gt; 192 0.257 0.743 #&gt; 193 0.683 0.317 #&gt; 194 0.257 0.743 #&gt; 195 0.257 0.743 #&gt; 196 0.257 0.743 #&gt; 197 0.683 0.317 #&gt; 198 0.683 0.317 #&gt; 199 0.257 0.743 #&gt; 200 0.683 0.317 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
