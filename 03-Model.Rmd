# Modelling

```{r, warning = FALSE}
# Upload the data
setwd(here::here("data/"))
GermanCredit <- read_csv("GermanCreditClean.csv")
```

```{r, results = 'hide'}
# Function to display nice confusion matrix
draw_confusion_matrix <- function(cm) {
  
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
  
  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Bad', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Good', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Bad', cex=1.2, srt=90)
  text(140, 335, 'Good', cex=1.2, srt=90)
  
  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
  
  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  
  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}
```

After cleaning and visualizing the data set, we move on to the creation of models.

## Splitting strategies and balancing 

In machine learning, a method to measure the accuracy of the models is to split the data into a training and a test set. The first subset is a portion of our data set that is fed into the machine learning model to discover and learn patterns. The other subset is to test our model. We split our data set as follows:

- **Training set:** 80% of the data
- **Test set:** the remaining 20% of the data

```{r, echo = TRUE}
# Splitting

set.seed(346)
# Creation of the index
index.tr <- createDataPartition(y = GermanCredit$RESPONSE, p= 0.8, list = FALSE) 

GermanCredit.tr <- GermanCredit[index.tr,] # Training set
GermanCredit.te <- GermanCredit[-index.tr,] # Testing set
```

<br>

As said in the exploratory data analysis, we notice that our data is heavily unbalanced.

```{r, echo = FALSE}
table(GermanCredit.tr$RESPONSE) %>% kable(align = "c", col.names = c("Outcome", "Frequence"))
```

<br>

Therefore, any model that favors the majority will reach an higher accuracy. However, in our case, we need to make sure to rightly classify any credit applications to avoid losses. To do so, we use a method called sub-sampling that balance the observations. It will allow us to have two equivalent samples. 


```{r, echo = TRUE}
# Balancing

n_no <- min(table(GermanCredit.tr$RESPONSE))

GermanCredit.tr.no <- filter(GermanCredit.tr, as_factor(RESPONSE)==0)
GermanCredit.tr.yes <- filter(GermanCredit.tr, as_factor(RESPONSE)==1)

## sub-sample 236 instances from the "Good"
index.no <- sample(size=n_no, x=1:nrow(GermanCredit.tr.no), replace=FALSE) 

## Bind all the "Bad" and the sub-sampled "Good"

GermanCredit.tr.subs <- data.frame(rbind(GermanCredit.tr.no, GermanCredit.tr.yes[index.no,])) 
```

<br>

By doing so, we get the following sub sampled training set:

```{r, echo = FALSE}
table(GermanCredit.tr.subs$RESPONSE) %>% kable(align = "c", col.names = c("Outcome", "Frequence"))
```


## K-Nearest Neighbors (K-NN)

A K-Nearest Neighbors tries to predict the correct class for the test data by calculating the distances between the test data and all the training points. Then to predict, it selects a number K, of the closest point of the test set (thus the name K-nearest neighbors).

```{r, echo = TRUE}
# K-Nearest Neighbors (K-NN)

trctrl <- trainControl(method = "cv", number=10) # Cross-validation
search_grid <- expand.grid(k = seq(1, 85, by = 1))
  
set.seed(346)
knn_cv <- train(as_factor(RESPONSE)~.,
                data = GermanCredit.tr.subs,
                method = "knn",
                trControl = trctrl,
                metric = "Accuracy",
                tuneGrid = search_grid)
```

```{r, results = 'hide', include = FALSE}
K_optimal <- knn_cv$finalModel$k
```

<br>

We need to select the K number of points closest that give the optimal accuracy. In this case, it is `r K_optimal`

```{r, results = 'hide'}
plot(knn_cv)
```
We did several confusion matrix to measure the performance. The first one was with the K-NNk method. We found an accuracy of 0.615 which corresponds to well predicted clients (good and bad). The best accuracy value is 1 so 0.615 is considered as an average value. 

<br>

```{r, results = 'hide'}
pred_knn <- predict(knn_cv, newdata = GermanCredit.te)
cmknn <- confusionMatrix(data = as.factor(pred_knn), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cmknn)
```

## Naive Bayes

Then, we used a probabilistic approach with Bayes classifiers. With this method, the confusion matrix provides a better accuracy of 0.72. 

```{r echo=TRUE, warning=FALSE}
# Naive Bayes

trctrl <- trainControl(method = "cv", number=10)

search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  laplace = 0:5,
  adjust = seq(0, 5, by = 1)
)

set.seed(346)
naive_bayes <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "naive_bayes",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

<br>

```{r, results = 'hide'}
pred_naive_bayes <-  predict(naive_bayes, newdata = GermanCredit.te)
cm_naive_bayes <- confusionMatrix(data = as.factor(pred_naive_bayes), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_naive_bayes)
```

## Logistic Regression

A logistic regression is a regression adapted to binary classification.

We use a cross-validation method to train our model and choose the AkaÃ¯ke Information Criterion (AIC) to select the variables. The AIC is used to select the model based on the number of  parameters. We choose the model with the smallest AIC.

```{r, echo = TRUE}
# Logistic Regression

trctrl <- trainControl(method = "cv", number=10)

set.seed(346)
glm_aic <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "glmStepAIC",
                 family="binomial",
                 trControl=trctrl,
                 trace=0)
```

The following plot corresponds to the resulting confusion matrix of the logistic regression:

<br> 

```{r}
pred_glm_aic <-  predict(glm_aic, newdata = GermanCredit.te)
cm_glm_aic <- confusionMatrix(data = as.factor(pred_glm_aic), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_glm_aic)
```

It provides an accuracy of 0.73.

## Linear Discriminant Analysis

```{r, echo = TRUE, results = 'hide'}
# Linear Discriminant Analysis
trctrl <- trainControl(method = "cv", number=10)

set.seed(346)
lda.model <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "lda",
                 trControl = trctrl)
```

<br>

```{r}
pred_lda <-  predict(lda.model, newdata = GermanCredit.te)
cm_lda <- confusionMatrix(data = as.factor(pred_lda), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_lda)
```

## Trees

The trees represent a hierarchical set of binary rules in a shape of a tree.

```{r, echo=TRUE}
# Trees

trctrl <- trainControl(method = "cv", number=10)
search_grid <- expand.grid(cp = seq(from = 0.1, to = 0, by = -0.01))

set.seed(346)
tree_model <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "rpart",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

<br>

```{r}
fancyRpartPlot(tree_model$finalModel, caption = NULL)
```

<br>

```{r}
pred_tree <-  predict(tree_model, newdata = GermanCredit.te)
cm_tree <- confusionMatrix(data = as.factor(pred_tree), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_tree)
```

## Neural Network

Neural Network is a method based on combining several predictions of small nodes. 

```{r, echo = TRUE, results = 'hide'}
# Neural Network (NN)

trctrl <- trainControl(method = "cv", number=5)
search_grid <- expand.grid(size = 1:10,
                           decay = seq(0, 0.5, 0.1))

set.seed(346)
neural_network <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "nnet",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

<br>

```{r}
pred_neural_network <-  predict(neural_network, newdata = GermanCredit.te)
cm_neural_network <- confusionMatrix(data = as.factor(pred_neural_network), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_neural_network)
```

The confusion matrix with neural network presents a good accuracy of 0.705.

## Random Forest

```{r, echo = TRUE, results = 'hide'}
# Random Forest

trctrl <- trainControl(method = "cv", number=5) 
search_grid <- expand.grid(.mtry = c(1:15)) 

set.seed(346)
rf <- train(as_factor(RESPONSE) ~., 
            data = GermanCredit.tr.subs,
            method = "rf",
            trControl = trctrl,
            tuneGrid = search_grid
)
```

<br>

```{r}
pred_rf <-  predict(rf, newdata = GermanCredit.te)
cm_rf <- confusionMatrix(data = as.factor(pred_rf), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_rf)
```
Again, this models shares an accuracy of 0.73.


## Summary

To evaluate our models, there are many metrics that can be used. We decided to choose first the accuracy and then have a look at the sensitivity and the specificity. In other words, we want to have a model with a good accuracy and the best sensitivity and specificity.

```{r, results = 'hide'}
accuracy_tree = cm_svm$overall['Accuracy']
accuracy_nn = cm_neural_network$overall['Accuracy']
accuracy_knn = cmknn$overall['Accuracy']
accuracy_nb = cm_naive_bayes$overall['Accuracy']
accuracy_glm = cm_glm_aic$overall['Accuracy']
accuracy_rf = cm_rf$overall['Accuracy']
accuracy_lda = cm_lda$overall['Accuracy']

sensitivity_tree = cm_tree$byClass['Sensitivity']
sensitivity_nn = cm_neural_network$byClass['Sensitivity']
sensitivity_knn = cmknn$byClass['Sensitivity']
sensitivity_nb = cm_naive_bayes$byClass['Sensitivity']
sensitivity_glm = cm_glm_aic$byClass['Sensitivity']
sensitivity_rf = cm_rf$byClass['Sensitivity']
sensitivity_lda = cm_lda$byClass['Sensitivity']

specificity_tree = cm_tree$byClass['Specificity']
specificity_nn = cm_neural_network$byClass['Specificity']
specificity_knn = cmknn$byClass['Specificity']
specificity_nb = cm_naive_bayes$byClass['Specificity']
specificity_glm = cm_glm_aic$byClass['Specificity']
specificity_rf = cm_rf$byClass['Specificity']
specificity_lda = cm_lda$byClass['Specificity']
```

<br>

```{r, echo = FALSE}
# Create a table to show the variables
metrics = c(accuracy_tree, sensitivity_tree, specificity_tree, accuracy_nn, sensitivity_nn, specificity_nn, accuracy_knn, sensitivity_knn, specificity_knn, accuracy_nb, sensitivity_nb, specificity_nb, accuracy_glm, sensitivity_glm, specificity_glm, accuracy_rf, sensitivity_rf, specificity_rf, accuracy_lda, sensitivity_lda, specificity_lda)
cnames = c("Accuracy", "Sensitivity", "Specificity")
rnames = c("Tree", "Neural Network", "K-nearest neighbors", "Naive Bayes", "Logistic Regression", "Random Forest", "Linear Discriminant Analysis")
model_output <- matrix(metrics, ncol = 3, byrow=TRUE, dimnames=list(rnames,cnames))
model_output <- model_output[order(model_output[,1],decreasing=TRUE),]

kable(model_output, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)
```

<br>

By looking at this summary table of the models, we can see that the Tree, the Logistic Regression and the Random Forest are our top models. However, the tree has a lowest score in terms of specificity and sensitivity than the two others and by far. For this reason, we decided to choose the following models:

- **Logistic Regression**
- **Random Forest**

## Variable Importance

The following plot shows a selection of the most important variables of our data frame.
The two main disadvantages of these methods are: The increasing over fitting risk when the number of observations is insufficient. The significant computation time when the number of variables is large.

The result below displays that we seem to have 6 relevant variables: checking account, the duration of the credit asked, the history of previous credits, the savings, the amount of money owned by the applicant and the real estate owned. 

```{r, message = FALSE, warning = FALSE}
# transform table in matrix
GermanCredit <- data.frame(GermanCredit)
GermanCredit <- apply(as.matrix.noquote(GermanCredit),2,as.numeric)

# select important variables
control <- trainControl(method="repeatedcv", number=10, repeats=2)

# train the model
model <- train(RESPONSE~., data=GermanCredit, method="rpart", preProcess="scale", trControl=control)

# estimate variable importance
importance <- varImp(model, scale=FALSE)

# plot importance
plot(importance)

# comment: we should find a way to print only positive results or to print only 10 firsts variables. 
```

After this first global analysis, we wanted to evaluate if there is room for improvement for the models selected in Machine Learning, in other words, if we can remove some variables to make the analysis easier without loosing the performance.

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}

explainer_glm <- DALEX::explain(model = glm_aic,
                                 data = GermanCredit.tr.subs, 
                                 y = GermanCredit.tr.subs$RESPONSE,
                                 label = "Logistic Regression")
```

```{r, echo = FALSE}
calculate_importance <- function(your_model_explainer, n_permutations = 10) {
  imp <- model_parts(explainer = your_model_explainer,
                     B = n_permutations,
                     type = "ratio",
                     N = NULL)
  return(imp)
}
```

### Logistic Regression

```{r, echo = FALSE}
importance_glm  <- calculate_importance(explainer_glm)

a <- plot(importance_glm) +
  ggtitle("Mean variable-importance ratio over 10 permutations", "")
a
```

```{r, echo = FALSE}
# Logistic Regression

trctrl <- trainControl(method = "cv", number=10)

set.seed(346)
glm_aic_vi <- train(as_factor(RESPONSE) ~ CHK_ACCT + HISTORY + INSTALL_RATE + DURATION + AMOUNT,
                 data = GermanCredit.tr.subs,
                 method = "glmStepAIC",
                 family="binomial",
                 trControl=trctrl,
                 trace=0)
```

<br>

```{r, results = 'hide'}
pred_glm_aic_vi <-  predict(glm_aic_vi, newdata = GermanCredit.te)
cm_glm_aic_vi <- confusionMatrix(data = as.factor(pred_glm_aic_vi), reference = as.factor(GermanCredit.te$RESPONSE))
```

```{r}
accuracy_glm_aic_vi = cm_glm_aic_vi$overall['Accuracy']
sensitivity_glm_aic_vi = cm_glm_aic_vi$byClass['Sensitivity']
specificity_glm_aic_vi = cm_glm_aic_vi$byClass['Specificity']
```

```{r}
# Create a table to compare variable importance versus before
metrics = c(accuracy_glm, sensitivity_glm, specificity_glm, accuracy_glm_aic_vi, sensitivity_glm_aic_vi, specificity_glm_aic_vi)
cnames = c("Accuracy", "Sensitivity", "Specificity")
rnames = c("Original Logistic Regression", "Refit Logistic Regression")
model_output <- matrix(metrics, ncol = 3, byrow=TRUE, dimnames=list(rnames,cnames))
model_output <- model_output[order(model_output[,1],decreasing=TRUE),]

kable(model_output, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)
```

<br>

As we can see in the table above, we lose performance in terms of the metrics. This means that the variable importance is not that important for the logistic regression. A good point is that we need to remind ourselves that the logistic regression already does a variable importance with the AIC criterion.

### Random Forest

```{r, echo = FALSE, results = 'hide'}
explainer_rf <- DALEX::explain(model = rf,
                                data = GermanCredit.tr.subs,
                                y = GermanCredit.tr.subs$RESPONSE,
                                label = "Random Forest")
```

```{r}
importance_rf <- calculate_importance(explainer_rf)

b <- plot(importance_rf) +
  ggtitle("Mean variable-importance ratio over 10 permutations", "")
b
```

```{r, echo = FALSE}
# Random Forest Refit

trctrl <- trainControl(method = "cv", number=5) 
opti_grid <- expand.grid(.mtry = rf$bestTune$mtry) 

set.seed(346)
rf_vi <- train(as_factor(RESPONSE) ~ CHK_ACCT + INSTALL_RATE + DURATION + SAV_ACCT + HISTORY,
            data = GermanCredit.tr.subs,
            method = "rf",
            trControl = trctrl,
            tuneGrid = opti_grid
)
```

```{r, results = 'hide'}
pred_rf_vi <-  predict(rf_vi, newdata = GermanCredit.te)
cm_rf_vi <- confusionMatrix(data = as.factor(pred_rf_vi), reference = as.factor(GermanCredit.te$RESPONSE))
```

```{r, echo = FALSE}
accuracy_rf_vi = cm_rf_vi$overall['Accuracy']
sensitivity_rf_vi = cm_rf_vi$byClass['Sensitivity']
specificity_rf_vi = cm_rf_vi$byClass['Specificity']
```

<br>

```{r}
# Create a table to compare variable importance versus before
metrics = c(accuracy_rf, sensitivity_rf, specificity_rf, accuracy_rf_vi, sensitivity_rf_vi, specificity_rf_vi)
cnames = c("Accuracy", "Sensitivity", "Specificity")
rnames = c("Original Random Forest", "Refit Random Forest")
model_output <- matrix(metrics, ncol = 3, byrow=TRUE, dimnames=list(rnames,cnames))
model_output <- model_output[order(model_output[,1],decreasing=TRUE),]

kable(model_output, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)
```

<br> 

Again, we lose a bit of performance in terms of the metrics. However, the model is simpler and might be easier to understand.

