# Modelling

```{r, warning = FALSE}
# Upload the data
setwd(here::here("data/"))
GermanCredit <- read_csv("GermanCreditClean.csv")
```

```{r, results = 'hide'}
# Function to display nice confusion matrix
draw_confusion_matrix <- function(cm) {
  
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
  
  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Bad', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Good', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Bad', cex=1.2, srt=90)
  text(140, 335, 'Good', cex=1.2, srt=90)
  
  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
  
  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  
  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}
```

Now that our data set is cleaned, we move on to the creation of models.

## Splitting strategies and balancing 

In machine learning, a method to measure the accuracy of the models is to split the data into a training and a test set. The first subset is a portion of our data set that is fed into the machine learning model to discover and learn patterns. The other subset is to test our model. We split our data set as follows:

- **Training set:** 80% of the data
- **Test set:** the remaining 20% of the data

```{r, echo = TRUE}
# Splitting

set.seed(346)
# Creation of the index
index.tr <- createDataPartition(y = GermanCredit$RESPONSE, p= 0.8, list = FALSE) 

GermanCredit.tr <- GermanCredit[index.tr,] # Training set
GermanCredit.te <- GermanCredit[-index.tr,] # Testing set
```

<br>

As said in the exploratory data analysis, we notice that our data is heavily unbalanced.

```{r, echo = FALSE}
table(GermanCredit.tr$RESPONSE) %>% kable(align = "c", col.names = c("Outcome", "Frequence"))
```

<br>

Therefore, any model that favors the majority will reach an higher accuracy. However, in our case, we need to make sure to rightly classify any credit applications to avoid losses. To do so, we use a method called sub-sampling that balance the observations.


```{r, echo = TRUE}
# Balancing

n_no <- min(table(GermanCredit.tr$RESPONSE))

GermanCredit.tr.no <- filter(GermanCredit.tr, as_factor(RESPONSE)==0)
GermanCredit.tr.yes <- filter(GermanCredit.tr, as_factor(RESPONSE)==1)

## sub-sample 236 instances from the "Good"
index.no <- sample(size=n_no, x=1:nrow(GermanCredit.tr.no), replace=FALSE) 

## Bind all the "Bad" and the sub-sampled "Good"

GermanCredit.tr.subs <- data.frame(rbind(GermanCredit.tr.no, GermanCredit.tr.yes[index.no,])) 
```

<br>

By doing so, we get the following subsampled training set:

```{r, echo = FALSE}
table(GermanCredit.tr.subs$RESPONSE) %>% kable(align = "c", col.names = c("Outcome", "Frequence"))
```


## K-Nearest Neighbors (K-NN)

A K-Nearest Neighbors tries to predict the correct class for the test data by calculating the distances between the test data and all the training points. Then to predict, it selects a number, K, of the closest point of the test set (thus the name K-nearest neighbors).

```{r, echo = TRUE}
# K-Nearest Neighbors (K-NN)

trctrl <- trainControl(method = "cv", number=10) # Cross-validation
search_grid <- expand.grid(k = seq(1, 85, by = 1))
  
set.seed(346)
knn_cv <- train(as_factor(RESPONSE)~.,
                data = GermanCredit.tr.subs,
                method = "knn",
                trControl = trctrl,
                metric = "Accuracy",
                tuneGrid = search_grid)
```

```{r, results = 'hide'}
K_optimal <- knn_cv$finalModel$k
```

<br>

We need to select the K number of points closest that give the optimal accuracy. In this case, it is `r K_optimal`

```{r, results = 'hide'}
plot(knn_cv)
```

```{r, results = 'hide'}
pred_knn <- predict(knn_cv, newdata = GermanCredit.te)
cmknn <- confusionMatrix(data = as.factor(pred_knn), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cmknn)
```

## Naive Bayes

Bayes classifiers use a probabilistic approach

```{r, warning = FALSE, echo = TRUE}
# Naive Bayes

trctrl <- trainControl(method = "cv", number=10)

search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  laplace = 0:5,
  adjust = seq(0, 5, by = 1)
)

set.seed(346)
naive_bayes <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "naive_bayes",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

```{r, results = 'hide'}
pred_naive_bayes <-  predict(naive_bayes, newdata = GermanCredit.te)
cm_naive_bayes <- confusionMatrix(data = as.factor(pred_naive_bayes), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_naive_bayes)
```

## Logistic Regression

A logistic regression is a regression adapted to binary classification.

We use a cross-validation method to train our model and choose the AkaÃ¯ke Information Criterion (AIC) to select the variables.

```{r, echo = TRUE}
# Logistic Regression

trctrl <- trainControl(method = "cv", number=10)

set.seed(346)
glm_aic <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "glmStepAIC",
                 family="binomial",
                 trControl=trctrl,
                 trace=0)
```

The resulting confusion matrix of the logistic regression:

```{r}
pred_glm_aic <-  predict(glm_aic, newdata = GermanCredit.te)
cm_glm_aic <- confusionMatrix(data = as.factor(pred_glm_aic), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_glm_aic)
```

## Trees

The trees represent a hierarchical set of binary rules in a shape of a tree.

```{r}
# Trees

trctrl <- trainControl(method = "cv", number=10)
search_grid <- expand.grid(cp = seq(from = 0.1, to = 0, by = -0.01))

set.seed(346)
tree_model <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "rpart",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

```{r}
fancyRpartPlot(tree_model$finalModel, caption = NULL)
```

```{r}
pred_tree <-  predict(tree_model, newdata = GermanCredit.te)
cm_tree <- confusionMatrix(data = as.factor(pred_tree), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_tree)
```

## Neural Network

Neural Network is a method based on combining several predictions of small nodes. 

```{r, echo = TRUE, results = 'hide'}
# Neural Network (NN)

trctrl <- trainControl(method = "cv", number=5)
search_grid <- expand.grid(size = 1:10,
                           decay = seq(0, 0.5, 0.1))

set.seed(346)
neural_network <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "nnet",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

```{r}
pred_neural_network <-  predict(neural_network, newdata = GermanCredit.te)
cm_neural_network <- confusionMatrix(data = as.factor(pred_neural_network), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_neural_network)
```

## Support Vector Machines

```{r, echo = TRUE, results = 'hide'}
# Support Vector Machines

trctrl <- trainControl(method = "cv", number=5)
search_grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))

set.seed(346)
svm <- train(as_factor(RESPONSE) ~.,
             data = GermanCredit.tr.subs,
             method = "svmLinear",
             trControl=trctrl,
             tuneGrid = search_grid)
```

```{r}
pred_svm <-  predict(svm, newdata = GermanCredit.te)
cm_svm <- confusionMatrix(data = as.factor(pred_svm), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_svm)
```

## Random Forest

```{r, echo = TRUE, results = 'hide'}
# Random Forest

trctrl <- trainControl(method = "cv", number=5) 
search_grid <- expand.grid(.mtry = c(1:15)) 

set.seed(346)
rf <- train(as_factor(RESPONSE) ~., 
            data = GermanCredit.tr.subs,
            method = "rf",
            trControl = trctrl,
            tuneGrid = search_grid
)
```

```{r}
pred_rf <-  predict(rf, newdata = GermanCredit.te)
cm_rf <- confusionMatrix(data = as.factor(pred_rf), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_rf)
```

## Summary

To evaluate our models, there are many metrics that can be used. We decided to choose first the accuracy and then have a look at the sensitivity and the specificity. In other words, we want to have a model with a good accuracy and the best sensitivity and specificity.

```{r, results = 'hide'}
accuracy_svm = cm_svm$overall['Accuracy']
accuracy_tree = cm_svm$overall['Accuracy']
accuracy_nn = cm_neural_network$overall['Accuracy']
accuracy_knn = cmknn$overall['Accuracy']
accuracy_nb = cm_naive_bayes$overall['Accuracy']
accuracy_glm = cm_glm_aic$overall['Accuracy']
accuracy_rf = cm_rf$overall['Accuracy']

sensitivity_svm = cm_svm$byClass['Sensitivity']
sensitivity_tree = cm_tree$byClass['Sensitivity']
sensitivity_nn = cm_neural_network$byClass['Sensitivity']
sensitivity_knn = cmknn$byClass['Sensitivity']
sensitivity_nb = cm_naive_bayes$byClass['Sensitivity']
sensitivity_glm = cm_glm_aic$byClass['Sensitivity']
sensitivity_rf = cm_rf$byClass['Sensitivity']

specificity_svm = cm_svm$byClass['Specificity']
specificity_tree = cm_tree$byClass['Specificity']
specificity_nn = cm_neural_network$byClass['Specificity']
specificity_knn = cmknn$byClass['Specificity']
specificity_nb = cm_naive_bayes$byClass['Specificity']
specificity_glm = cm_glm_aic$byClass['Specificity']
specificity_rf = cm_rf$byClass['Specificity']
```

```{r, echo = FALSE}
# Create a table to show the variables
metrics = c(accuracy_svm, sensitivity_svm, specificity_svm, accuracy_tree, sensitivity_tree, specificity_tree, accuracy_nn, sensitivity_nn, specificity_nn, accuracy_knn, sensitivity_knn, specificity_knn, accuracy_nb, sensitivity_nb, specificity_nb, accuracy_glm, sensitivity_glm, specificity_glm, accuracy_rf, sensitivity_rf, specificity_rf)
cnames = c("Accuracy", "Sensitivity", "Specificity")
rnames = c("Support Vector Machines", "Tree", "Neural Network", "K-nearest neighbors", "Naive Bayes", "Logistic Regression", "Random Forest")
model_output <- matrix(metrics, ncol = 3, byrow=TRUE, dimnames=list(rnames,cnames))
model_output <- model_output[order(model_output[,1],decreasing=TRUE),]

kable(model_output, digits = 2)  %>% 
  kable_styling(bootstrap_options = c("striped","condensed"), fixed_thead = T)
```

<br>

By looking at this summary table of the models, we can see that Support Vector Machines, Tree and Logistic Regression are our top models. However, in terms of sensitivity and specificity, two models outpeform the other. As a reminder, in our case, we want to be sure to have an balanced sensitivity and specificity. Therefore, we take only the two models for the following part: 

- Support Vector Machines
- Logistic Regression
