# Models

```{r, warning = FALSE}
# Upload the data
setwd(here::here("data/"))
GermanCredit <- read_csv("GermanCreditClean.csv")
```

```{r, results = 'hide'}
# Function to display nice confusion matrix
draw_confusion_matrix <- function(cm) {
  
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
  
  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Bad', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Good', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Bad', cex=1.2, srt=90)
  text(140, 335, 'Good', cex=1.2, srt=90)
  
  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
  
  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  
  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}
```

Now that our data set is cleaned, we move on to the creation of models to predict our outcome.

## Splitting strategies and balancing 

In machine learning, a method to measure the accuracy of the models is to split the data into a training and a test set. The first subset is a portion of our data set that is fed into the machine learning model to discover and learn patterns. The other subset is to test our model. We split our data set as follows:

- **Training set:** 80% of the data
- **Test set:** the remaining 20% of the data

```{r, echo = TRUE}
# Splitting

set.seed(346)
# Creation of the index
index.tr <- createDataPartition(y = GermanCredit$RESPONSE, p= 0.8, list = FALSE) 

GermanCredit.tr <- GermanCredit[index.tr,] # Training set
GermanCredit.te <- GermanCredit[-index.tr,] # Testing set
```

As said in the exploratory data analysis, we notice that our data is heavily unbalanced.

```{r, echo = FALSE}
table(GermanCredit.tr$RESPONSE) %>% kable(align = "c", col.names = c("Outcome", "Frequence"))
```

Therefore, any model that favors the majority will reach an higher accuracy. However, in our case, we need to make sure to rightly classify any credit applications to avoid losses. To do so, we use a method called sub-sampling that balance the observations.


```{r, echo = TRUE}
# Balancing

n_no <- min(table(GermanCredit.tr$RESPONSE))

GermanCredit.tr.no <- filter(GermanCredit.tr, as_factor(RESPONSE)==0)
GermanCredit.tr.yes <- filter(GermanCredit.tr, as_factor(RESPONSE)==1)

## sub-sample 236 instances from the "Good"
index.no <- sample(size=n_no, x=1:nrow(GermanCredit.tr.no), replace=FALSE) 

## Bind all the "Bad" and the sub-sampled "Good"

GermanCredit.tr.subs <- data.frame(rbind(GermanCredit.tr.no, GermanCredit.tr.yes[index.no,])) 
```

```{r, echo = FALSE}
table(GermanCredit.tr.subs$RESPONSE) %>% kable(align = "c", col.names = c("Outcome", "Frequence"))
```


## K-Nearest Neighbors (K-NN)

A K-Nearest Neighbors tries to predict the correct class for the test data by calculating the distances between the test data and all the training points. Then select a number, K, of the closest point of the test set (thus the name K-nearest neighbors).

```{r, echo = TRUE}
trctrl <- trainControl(method = "cv", number=10)
search_grid <- expand.grid(k = seq(1, 85, by = 1))
  
set.seed(346)
knn_cv <- train(as_factor(RESPONSE)~.,
                data = GermanCredit.tr.subs,
                method = "knn",
                trControl = trctrl,
                metric = "Accuracy",
                tuneGrid = search_grid)
```

We need to select the K number of points closest that give the optimal accuracy

```{r, results = 'hide'}
K_optimal <- knn_cv$finalModel$k
```


```{r, results = 'hide'}
plot(knn_cv)
```

```{r, results = 'hide'}
pred_knn <- predict(knn_cv, newdata = GermanCredit.te)
cmknn <- confusionMatrix(data = as.factor(pred_knn), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cmknn)
```

## Naive Bayes

Bayes classifiers use a probabilistic approach

```{r, warning = FALSE, echo = TRUE}
trctrl <- trainControl(method = "cv", number=10)

search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  laplace = 0:5,
  adjust = seq(0, 5, by = 1)
)

set.seed(346)
naive_bayes <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "naive_bayes",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

```{r, results = 'hide'}
pred_naive_bayes <-  predict(naive_bayes, newdata = GermanCredit.te)
cm_naive_bayes <- confusionMatrix(data = as.factor(pred_naive_bayes), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_naive_bayes)
```

## Logistic Regression

A logistic regression is a regression adapted to binary classification.

We use a cross-validation method to train our model and choose the AkaÃ¯ke Information Criterion (AIC) to select the variables.

```{r, echo = TRUE}
trctrl <- trainControl(method = "cv", number=10)

set.seed(346)
glm_aic <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "glmStepAIC",
                 family="binomial",
                 trControl=trctrl,
                 trace=0)
```

The resulting confusion matrix of the logistic regression:

```{r}
pred_glm_aic <-  predict(glm_aic, newdata = GermanCredit.te)
cm_glm_aic <- confusionMatrix(data = as.factor(pred_glm_aic), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_glm_aic)
```

## Trees

The trees represent a hierarchical set of binary rules in a shape of a tree.

```{r}
trctrl <- trainControl(method = "cv", number=10)
search_grid <- expand.grid(cp = seq(from = 0.1, to = 0, by = -0.01))

set.seed(346)
tree_model <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "rpart",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

```{r}
fancyRpartPlot(tree_model$finalModel, caption = NULL)
```

```{r}
pred_tree <-  predict(tree_model, newdata = GermanCredit.te)
cm_tree <- confusionMatrix(data = as.factor(pred_tree), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_tree)
```

## Neural Network

Neural Network is a method based on combining several predictions of small nodes. 

```{r}
trctrl <- trainControl(method = "cv", number=5)
search_grid <- expand.grid(size = 1:10,
                           decay = seq(0, 0.5, 0.1))

set.seed(346)
neural_network <- train(as_factor(RESPONSE) ~.,
                 data = GermanCredit.tr.subs,
                 method = "nnet",
                 trControl=trctrl,
                 tuneGrid = search_grid)
```

```{r}
pred_neural_network <-  predict(neural_network, newdata = GermanCredit.te)
cm_neural_network <- confusionMatrix(data = as.factor(pred_neural_network), reference = as.factor(GermanCredit.te$RESPONSE))
draw_confusion_matrix(cm_neural_network)
```
```




