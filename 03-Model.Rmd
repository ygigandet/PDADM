# Models

```{r, warning = FALSE}
# Upload the data
setwd(here::here("data/"))
GermanCredit <- read_csv("GermanCreditClean.csv")
```

After the exploration of our data, we start with the modelling part.

## Splitting strategies

In order to test the performance of our models, we use a splitting strategy of the dataset:

- 80% is used as a training set
- The remaining 20% is used as a test set.

```{r}
# Creation of the training and splitting data set
set.seed(346) # To get the same results
index.tr <- createDataPartition(y = GermanCredit$RESPONSE, p= 0.8, list = FALSE) #Creation of the index
GermanCredit.tr <- GermanCredit[index.tr,] # Training set
GermanCredit.te <- GermanCredit[-index.tr,] # Testing set
```

## Balance the dataset

As said in the exploratory data analysis, we notice that our data is heavily unbalanced. We need to balance it before fitting our model. 

```{r, results='hide'}
# Balancing the training set
table(GermanCredit.tr$RESPONSE)

n <- min(table(GermanCredit.tr$RESPONSE))

GermanCredit.tr.no <- filter(GermanCredit.tr, as_factor(RESPONSE)==0)
GermanCredit.tr.yes <- filter(GermanCredit.tr, as_factor(RESPONSE)==1)

index.no <- sample(size=n, x=1:nrow(GermanCredit.tr.no), replace=FALSE) ## sub-sample 236 instances from the "Yes"

GermanCredit.tr.subs <- data.frame(rbind(GermanCredit.tr.no, GermanCredit.tr.yes[index.no,])) ## Bind all the "No" and the sub-sampled "Yes"
table(GermanCredit.tr.subs$RESPONSE) ## The cases are balanced
```

## Logistic Regression

We fit a logistic regression to test if it could predict the output we desired.

For our first model we'll select all the variables given in the German Credit data set.

```{r}
mod.logr <- glm(as_factor(RESPONSE)~., data = GermanCredit.tr.subs, family="binomial")
summary(mod.logr)
```

Our initial model is therefore:

$$
z_i = a_o + a_1CHKACCT_{1i} + ... + a_{32}Foreign_{1i}
$$

Let's check if we can select better variables for the model

```{r}
# Using the AIC
mod.logr.sel <- step(mod.logr) # Here by default, backwards selection
summary(mod.logr.sel) # Reminder, the lower the AIC the better
```

Predictions: 

```{r}
prob.te <- predict(mod.logr.sel, newdata=GermanCredit.te, type="response")
pred.te <- ifelse(prob.te >= 0.5, 1, 0)
table(Pred=pred.te, Obs=GermanCredit.te$RESPONSE)
```

Not a good model. Do not forget that the prior of this data set is not balanced at all. We should take it into account.

As depicted below, if our model was good, we should have boxplot that are far away from the 0.5, which is not the case, especially for the bad credits.

```{r}
boxplot(prob.te~GermanCredit.te$RESPONSE)
```

### Cross-validation

```{r}
trctrl <- trainControl(method = "cv", number=10)

set.seed(346)
GermanCredit.cv <- train(as_factor(RESPONSE) ~., data = GermanCredit.tr.subs, method = "glmStepAIC", family="binomial",
                    trControl=trctrl, trace=0)

GermanCredit.cv
```

```{r}
GermanCredit.pred <- predict(GermanCredit.cv, newdata = GermanCredit.te)
confusionMatrix(data=GermanCredit.cv, reference = GermanCredit.te$RESPONSE)
```

